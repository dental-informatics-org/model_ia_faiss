import torch.multiprocessing as mp
import os
import torch
from transformers import AutoTokenizer, AutoModel
import faiss
import numpy as np
import pickle

mp.set_start_method('spawn', force=True)
# ‚úÖ Corrige erro CUDA no Celery

# Verifica se o c√≥digo est√° rodando dentro de um worker do Celery
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Caminhos dos arquivos
INDICE_PATH = "data/index/faiss_index"
ID_PARA_TEXTO_PATH = "data/index/id_para_texto.pkl"
dimension = 768  # Dimens√£o dos embeddings

# Criar diret√≥rio se n√£o existir
os.makedirs(os.path.dirname(INDICE_PATH), exist_ok=True)

# Inicializa dicion√°rio
global id_para_texto
id_para_texto = {}

# Carregar modelo e tokenizer apenas uma vez
tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')
model = AutoModel.from_pretrained('sentence-transformers/paraphrase-multilingual-mpnet-base-v2').to(device)

def gerar_embeddings(texto):
    tokens = tokenizer(texto, return_tensors='pt', truncation=True, padding=True)
    tokens = {k: v.to(device) for k, v in tokens.items()}
    
    with torch.no_grad():
        output = model(**tokens)

    token_embeddings = output.last_hidden_state
    attention_mask = tokens["attention_mask"].unsqueeze(-1).expand(token_embeddings.shape).float()
    emb = (token_embeddings * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)
    emb = emb.cpu().numpy().astype(np.float32)

    faiss.normalize_L2(emb)  # Normalizar embeddings antes de retornar
    return emb

def carregar_id_para_texto():
    global id_para_texto
    if os.path.exists(ID_PARA_TEXTO_PATH):
        with open(ID_PARA_TEXTO_PATH, "rb") as f:
            id_para_texto = pickle.load(f)
        print(f"Carregado id_para_texto com {len(id_para_texto)} itens.")
    else:
        id_para_texto = {}

def salvar_id_para_texto():
    with open(ID_PARA_TEXTO_PATH, "wb") as f:
        pickle.dump(id_para_texto, f)

def carregar_indice():
    if os.path.exists(INDICE_PATH):
        index = faiss.read_index(INDICE_PATH)
        if index.d != dimension:
            raise ValueError(f"Dimens√£o do √≠ndice ({index.d}) n√£o corresponde ao esperado ({dimension})")
        return index
    else:
        print(f"‚ö†Ô∏è ERRO: O arquivo {INDICE_PATH} n√£o existe. Um novo √≠ndice ser√° criado.")
        quantizer = faiss.IndexFlatIP(dimension)
        index = faiss.IndexIVFFlat(quantizer, dimension, 100, faiss.METRIC_INNER_PRODUCT)
        return index

def indexar_textos(linhas):
    mp.set_start_method('spawn', force=True)
    global id_para_texto, index  # <- Aqui est√° correto

    embeddings = [gerar_embeddings(linha) for linha in linhas]
    embeddings = np.vstack(embeddings)  # Empilha os arrays
    faiss.normalize_L2(embeddings)  # üîπ Normaliza antes de adicionar

    if embeddings.shape[1] != dimension:
        raise ValueError(f"Dimens√£o dos embeddings ({embeddings.shape[1]}) n√£o corresponde ao esperado ({dimension})")

    # üîπ Se o √≠ndice n√£o foi treinado, treine antes de adicionar os embeddings
    if not index.is_trained:
        print("‚ö†Ô∏è √çndice FAISS ainda n√£o treinado. Treinando agora...")
        index.train(embeddings)
        print("‚úÖ Treinamento conclu√≠do.")

    index.add(embeddings)  # Adiciona os embeddings ao √≠ndice

    # üîπ Salvar ID -> texto
    novo_id = len(id_para_texto)
    for i, linha in enumerate(linhas):
        id_para_texto[novo_id + i] = linha
    
    salvar_id_para_texto()
    faiss.write_index(index, INDICE_PATH)
    print(f"‚úÖ √çndice FAISS salvo em {INDICE_PATH}")

    # üîπ For√ßar recarregamento do √≠ndice
    index = carregar_indice()  # üîÑ Atualiza a vari√°vel global
    print("üîÑ √çndice FAISS recarregado com sucesso.")


def buscar_textos(consulta, top_k=5):
    embedding = gerar_embeddings(consulta)
    faiss.normalize_L2(embedding)
    distancias, indices = index.search(embedding, top_k)
    
    resultados = []
    for i, idx in enumerate(indices[0]):
        if idx in id_para_texto:
            resultados.append({
                "texto": id_para_texto[idx],
                "distancia": float(distancias[0][i])
            })
    return resultados

# Carregar os dados ao iniciar
index = carregar_indice()
carregar_id_para_texto()
print("‚úÖ Sistema pronto para buscas!")
